{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# A Deep Learning Approach to Identifying Covert Disinformation Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Boilerplate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Suppress warning caused by h5py version Conda is using\n",
    "import warnings\n",
    "warnings.simplefilter(action='ignore', category=FutureWarning)\n",
    "\n",
    "from multiplicative_lstm import MultiplicativeLSTM\n",
    "import numpy as np\n",
    "import preProcess\n",
    "\n",
    "from keras.preprocessing import sequence\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Activation, Bidirectional, Conv1D, Dense, Dropout\n",
    "from keras.layers import Embedding, GlobalMaxPooling1D, LSTM, MaxPooling1D"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocess Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calls preprocessing routines to turn positive and negative datasets into train and test\n",
    "#   input and output vectors\n",
    "# Here, pos.csv comes from tweets sent by the Internet Research Agency\n",
    "#   neg.csv is a random set of tweets geolocated in the U.S.,\n",
    "#   neg3.csv is a set of tweets chosen to represent a similiar user and content makeup to pos.csv.\n",
    "# Details on datasets in the project paper\n",
    "def loadData():\n",
    "    POSFILE = \"datasets\\\\pos.csv\"\n",
    "    NEGFILE = \"datasets\\\\neg.csv\"\n",
    "    NEGFILE2 = \"datasets\\\\neg3.csv\"\n",
    "\n",
    "    MAXPOS = 200000\n",
    "    MAXNEG = 200000\n",
    "    MAXNEG2 = 0\n",
    "    GROUP = 20\n",
    "    TRAINPCT = .8\n",
    "\n",
    "    splitSize = int(TRAINPCT*((MAXPOS//GROUP + (MAXNEG + MAXNEG2)//GROUP)))\n",
    "    (x, y) = preProcess.readData(POSFILE, NEGFILE, NEGFILE2, MAXPOS, MAXNEG, MAXNEG2, GROUP)\n",
    "    indexes = preProcess.getIndexes(x)\n",
    "    (X, Y) = preProcess.vectorize(x, y, indexes)\n",
    "    (x_train, y_train), (x_test, y_test) = preProcess.splitData(X, Y, splitSize)\n",
    "    \n",
    "    return (x_train, y_train), (x_test, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, y_train), (x_test, y_test) = loadData()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first model tried was a modified version of the imdb_cnn_lstm example on the keras team's Github: \n",
    "https://github.com/keras-team/keras/blob/master/examples/imdb_cnn_lstm.py\n",
    "\n",
    "The multiplicative LSTM tested came from https://github.com/titu1994/Keras-Multiplicative-LSTM.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set hyperparameters\n",
    "max_features = max(np.amax(x_train), np.amax(x_test)) + 1\n",
    "embedding_size = 4\n",
    "maxlen = len(x_train[0])\n",
    "\n",
    "kernel_size = 16\n",
    "filters = 64\n",
    "pool_size = 4\n",
    "\n",
    "lstm_output_size = 70\n",
    "\n",
    "batch_size = 32\n",
    "epochs = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run model\n",
    "model = Sequential()\n",
    "\n",
    "model.add(Embedding(max_features, embedding_size, input_length=maxlen))\n",
    "model.add(Dropout(0.20))\n",
    "\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(Conv1D(filters, kernel_size, padding='valid', activation='relu', strides=1))\n",
    "model.add(MaxPooling1D(pool_size=pool_size))\n",
    "model.add(GlobalMaxPooling1D())\n",
    "\n",
    "# These were the models listed in the project paper that didn't work well\n",
    "#model.add(LSTM(lstm_output_size))\n",
    "#model.add(Bidirectional(LSTM(lstm_output_size)))\n",
    "#model.add(MultiplicativeLSTM(lstm_output_size, dropout=0.2, recurrent_dropout=0.2))\n",
    "#model.add(Bidirectional(MultiplicativeLSTM(lstm_output_size, dropout=0.2, recurrent_dropout=0.2)))\n",
    "\n",
    "model.add(Dense(1))\n",
    "model.add(Activation('sigmoid'))\n",
    "\n",
    "model.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "history = model.fit(x_train, y_train, batch_size=batch_size, epochs=epochs, validation_data=(x_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "score, acc = model.evaluate(x_test, y_test, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('History: ', history.history)\n",
    "print('Test score:', score)\n",
    "print('Test accuracy:', acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
